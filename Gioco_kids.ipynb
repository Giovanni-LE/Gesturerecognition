{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 21:20:51.337827: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import cvzone\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model (to make our detection)\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities (to draw them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "\n",
    "    \"\"\"\n",
    "    Esegue la rilevazione dei landmark delle mani utilizzando il modello di MediaPipe.\n",
    "\n",
    "    Input:\n",
    "    - image: l'immagine di input (frame) su cui effettuare la rilevazione dei landmark\n",
    "    - model: il modello di MediaPipe per la rilevazione dei landmark\n",
    "\n",
    "    Output:\n",
    "    - image: l'immagine di input modificata, con i landmark disegnati\n",
    "    - results: gli oggetti 'results' contenenti i risultati della rilevazione dei landmark\n",
    "    \"\"\"\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    Disegna i landmark delle mani sull'immagine utilizzando i risultati ottenuti dalla rilevazione di MediaPipe.\n",
    "\n",
    "    Input:\n",
    "    - image: l'immagine su cui disegnare i landmark\n",
    "    - results: i risultati ottenuti dalla rilevazione di MediaPipe\n",
    "\n",
    "    Output:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    # Disegna i landmark della mano sinistra e le relative connessioni\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "    # Disegna i landmark della mano destra e le relative connessioni\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    Disegna i landmark delle mani sull'immagine utilizzando uno stile personalizzato.\n",
    "\n",
    "    Input:\n",
    "    - image: l'immagine su cui disegnare i landmark\n",
    "    - results: i risultati ottenuti dalla rilevazione di MediaPipe\n",
    "\n",
    "    Output:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    # Disegna i landmark della mano sinistra e le relative connessioni con uno stile personalizzato\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "\n",
    "    # Disegna i landmark della mano destra e le relative connessioni con uno stile personalizzato\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    \"\"\"\n",
    "    Estrae i punti chiave dei landmark delle mani dai risultati di MediaPipe.\n",
    "\n",
    "    Input:\n",
    "    - results: i risultati ottenuti dalla rilevazione di MediaPipe\n",
    "\n",
    "    Output:\n",
    "    - keypoints: un array numpy contenente i punti chiave dei landmark delle mani\n",
    "    \"\"\"\n",
    "\n",
    "    # Estrae i punti chiave dei landmark della mano sinistra se disponibili, altrimenti crea un array di zeri\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "    # Estrae i punti chiave dei landmark della mano destra se disponibili, altrimenti crea un array di zeri\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "    # Concatena i punti chiave delle mani sinistra e destra in un unico array\n",
    "    keypoints = np.concatenate([lh, rh])\n",
    "\n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azioni da riconoscere\n",
    "actions = np.array(['0','1', '2', '3','4','5','altro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Import Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(126,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "model.load_weights('modello/action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245, 117, 16), (117, 245, 16), (16, 117, 245), (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0)]\n",
    "\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "\n",
    "    \"\"\"\n",
    "    La funzione visualizza una barra di probabilità colorata per ogni azione prevista.\n",
    "    Input:\n",
    "    - res: Array di probabilità delle azioni previste.\n",
    "    - actions: Array delle etichette delle azioni.\n",
    "    - input_frame: Frame di input su cui disegnare le barre di probabilità.\n",
    "    - colors: Lista dei colori corrispondenti alle azioni.\n",
    "    \n",
    "    Output:\n",
    "    - output_frame: Frame di output con le barre di probabilità disegnate.\n",
    "    \"\"\"\n",
    "\n",
    "    # Creazione di una copia del frame di input\n",
    "    output_frame = input_frame.copy()\n",
    "    \n",
    "    for num, prob in enumerate(res):\n",
    "        # Disegno del rettangolo colorato proporzionale alla probabilità dell'azione\n",
    "        cv2.rectangle(output_frame, (0, 60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        \n",
    "        # Aggiunta del testo dell'azione corrispondente sopra il rettangolo\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    return output_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 21:20:55.748 python[6616:349800] mac-virtualcam(DAL): PlugInMain version=1.3.0\n",
      "2023-06-19 21:20:55.748 python[6616:349800] mac-virtualcam(DAL): HardwarePlugIn_QueryInterface \n",
      "2023-06-19 21:20:55.748 python[6616:349800] mac-virtualcam(DAL): HardwarePlugIn_Release sRefCount now = 0\n",
      "2023-06-19 21:20:55.748 python[6616:349800] mac-virtualcam(DAL): HardwarePlugIn_InitializeWithObjectID self=0x138c40478\n",
      "2023-06-19 21:20:55.749 python[6616:349800] mac-virtualcam(DAL): HardwarePlugIn_ObjectSetPropertyData OBSDALDevice(33) kCMIOObjectPropertyListenerAdded self=0x138c40478 data(int)=1684629094\n",
      "2023-06-19 21:20:55.749 python[6616:349800] mac-virtualcam(DAL): HardwarePlugIn_ObjectSetPropertyData OBSDALDevice(33) kCMIOObjectPropertyListenerAdded self=0x138c40478 data(int)=1869180523\n",
      "2023-06-19 21:20:55.749 python[6616:349800] mac-virtualcam(DAL): HardwarePlugIn_ObjectSetPropertyData OBSDALDevice(33) kCMIOObjectPropertyListenerAdded self=0x138c40478 data(int)=1885762592\n",
      "2023-06-19 21:20:55.799 python[6616:349800] mac-virtualcam(DAL): HardwarePlugIn_ObjectSetPropertyData OBSDALDevice(33) kCMIOObjectPropertyListenerAdded self=0x138c40478 data(int)=1818850926\n",
      "2023-06-19 21:20:55.799 python[6616:349800] mac-virtualcam(DAL): HardwarePlugIn_ObjectSetPropertyData OBSDALDevice(33) kCMIOObjectPropertyListenerAdded self=0x138c40478 data(int)=1651797110\n",
      "2023-06-19 21:20:55.799 python[6616:349800] mac-virtualcam(DAL): HardwarePlugIn_ObjectSetPropertyData OBSDALDevice(33) kCMIOObjectPropertyListenerAdded self=0x138c40478 data(int)=1651798900\n",
      "2023-06-19 21:20:55.799 python[6616:349800] mac-virtualcam(DAL): HardwarePlugIn_ObjectSetPropertyData OBSDALDevice(33) kCMIOObjectPropertyListenerAdded self=0x138c40478 data(int)=1869180523\n",
      "2023-06-19 21:20:55.800 python[6616:349800] mac-virtualcam(DAL): HardwarePlugIn_ObjectSetPropertyData OBSDALDevice(33) kCMIOObjectPropertyListenerAdded self=0x138c40478 data(int)=1937009955\n",
      "2023-06-19 21:20:55.800 python[6616:349800] mac-virtualcam(DAL): HardwarePlugIn_ObjectSetPropertyData OBSDALDevice(33) kCMIOObjectPropertyListenerAdded self=0x138c40478 data(int)=1886156132\n",
      "2023-06-19 21:20:55.800 python[6616:349800] mac-virtualcam(DAL): HardwarePlugIn_ObjectSetPropertyData OBSDALDevice(33) kCMIOObjectPropertyListenerAdded self=0x138c40478 data(int)=1935833461\n",
      "2023-06-19 21:20:55.800 python[6616:349800] mac-virtualcam(DAL): HardwarePlugIn_ObjectSetPropertyData OBSDALStream(34) kCMIOObjectPropertyListenerAdded self=0x138c40478 data(int)=1885762592\n",
      "2023-06-19 21:20:55.801 python[6616:349800] mac-virtualcam(DAL): HardwarePlugIn_ObjectSetPropertyData OBSDALStream(34) kCMIOObjectPropertyListenerAdded self=0x138c40478 data(int)=1852207732\n",
      "2023-06-19 21:20:55.801 python[6616:349800] mac-virtualcam(DAL): HardwarePlugIn_ObjectSetPropertyData OBSDALStream(34) kCMIOObjectPropertyListenerAdded self=0x138c40478 data(int)=1835430516\n",
      "2023-06-19 21:20:55.851 python[6616:349836] mac-virtualcam(DAL): PlugIn unhandled hasPropertyWithAddress for Unknown selector: ddsc\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "altro\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "altro\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "altro\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "altro\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "altro\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "altro\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "altro\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "altro\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "altro\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "0\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "0\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "0\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "0\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "0\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "0\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "0\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "0\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "0\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "0\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "0\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "0\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "3\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "altro\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "altro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 21:21:15.716 python[6616:355192] mac-virtualcam(DAL): PlugIn unhandled hasPropertyWithAddress for Unknown selector: ddsc\n"
     ]
    }
   ],
   "source": [
    "# Funzione per caricare le immagini dei gesti\n",
    "def load_gesture_images():\n",
    "    \"\"\"Questa funzione carica le immagini dei gesti da una directory specifica. Restituisce una lista di immagini dei gesti.\n",
    "\n",
    "    Input:\n",
    "    - Nessun input richiesto.\n",
    "\n",
    "    Output:\n",
    "    - Una lista di immagini dei gesti.\n",
    "    \"\"\"\n",
    "    gesture_images = [] # Crea una lista vuota per le immagini dei gesti\n",
    "    for i in range(6): # Itera da 0 a 5 (6 iterazioni)\n",
    "        image_path = f\"gesture_images/{i}.png\" # Ottiene il percorso dell'immagine corrente\n",
    "        image = cv2.imread(image_path,cv2.IMREAD_UNCHANGED) # Carica l'immagine utilizzando OpenCV\n",
    "        gesture_images.append(image) # Aggiunge l'immagine alla lista delle immagini dei gesti\n",
    "    return gesture_images\n",
    "\n",
    "def show_message_on_image(image, message, color, position=None):\n",
    "    \"\"\"\n",
    "    Questa funzione mostra un messaggio sull'immagine specificata.\n",
    "\n",
    "    Input:\n",
    "    - image: L'immagine su cui mostrare il messaggio.\n",
    "    - message: Il messaggio da mostrare sull'immagine.\n",
    "    - color: Il colore del testo.\n",
    "    - position (opzionale): La posizione del testo sull'immagine. Se non specificato, il testo verrà centrato sull'immagine.\n",
    "\n",
    "    Output:\n",
    "    - Nessun output restituito. L'immagine viene modificata direttamente.\n",
    "\n",
    "    \"\"\"\n",
    "    font_scale = 1\n",
    "    font_thickness = 2\n",
    "    font_face = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    # Calcola le dimensioni del testo\n",
    "    (text_width, text_height), _ = cv2.getTextSize(message, font_face, font_scale, font_thickness)\n",
    "\n",
    "    # Calcola la posizione del testo in base alla posizione specificata o al centro dell'immagine\n",
    "    if position is None:\n",
    "        position = ((image.shape[1] - text_width) // 2, (image.shape[0] + text_height) // 2)\n",
    "\n",
    "    # Disegna il testo sull'immagine\n",
    "    cv2.putText(image, message, position, font_face, font_scale, color, font_thickness, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "\n",
    "# Nuove variabili per la rilevazione\n",
    "sequence = []  # Lista vuota per la sequenza dei gesti\n",
    "sentence = []  # Lista vuota per la frase composta dai gesti\n",
    "predictions = []  # Lista vuota per le predizioni\n",
    "threshold = 0.5  # Soglia di confidenza per considerare una predizione corretta\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # Inizializza la cattura video dalla webcam\n",
    "cap.set(3, 640)  # Imposta la larghezza del frame a 640 pixel\n",
    "cap.set(4, 480)  # Imposta l'altezza del frame a 480 pixel\n",
    "\n",
    "timer = 0  # Variabile per il timer\n",
    "stateResult = False  # Stato del risultato (se è stato mostrato un risultato)\n",
    "\n",
    "gesture_images = load_gesture_images()  # Carica le immagini dei gesti\n",
    "num_gestures = len(gesture_images)  # Numero totale di gesti\n",
    "current_gesture = 0  # Indice del gesto corrente\n",
    "prev_gesture = None  # Indice del gesto precedente\n",
    "exit_flag = False  # Flag di uscita\n",
    "game_started = False  # Flag per indicare se il gioco è iniziato\n",
    "indovinato = False  # Flag per indicare se il gesto è stato indovinato\n",
    "\n",
    "# Imposta il modello di MediaPipe per la rilevazione dei landmark\n",
    "with mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened() and not exit_flag:\n",
    "        imgBG = cv2.imread(\"BG.png\")  # Carica lo sfondo del gioco\n",
    "\n",
    "        # Mostra il messaggio \"Premi 'S' per iniziare\" finché il gioco non è iniziato\n",
    "        while not game_started:\n",
    "            ret, frame = cap.read()  # Leggi il frame dalla webcam\n",
    "            imgScaled = cv2.resize(frame, (0, 0), None, 0.875, 0.875)  # Ridimensiona il frame\n",
    "            imgScaled = imgScaled[:, 80:480]  # Ritaglia il frame\n",
    "            show_message_on_image(imgScaled, \"Press 'S' to start\", (255, 255, 255), position=(10, 30))  # Mostra messaggio personalizzato\n",
    "            show_message_on_image(imgScaled, \"START!\", (0, 255, 0))  # Mostra messaggio centrato\n",
    "            imgBG[234:654, 795:1195] = imgScaled  # Sovrapponi il frame allo sfondo\n",
    "            cv2.imshow(\"BG\", imgBG)  # Mostra l'immagine con lo sfondo\n",
    "\n",
    "            key = cv2.waitKey(1)\n",
    "            if key == ord('s'):\n",
    "                game_started = True\n",
    "                initialTime = time.time()\n",
    "\n",
    "        ret, frame = cap.read()  # Leggi il frame dalla webcam\n",
    "        imgScaled = cv2.resize(frame, (0, 0), None, 0.875, 0.875)  # Ridimensiona il frame\n",
    "        imgScaled = imgScaled[:, 80:480]  # Ritaglia il frame\n",
    "\n",
    "        # Effettua le rilevazioni dei landmark utilizzando il modello di MediaPipe\n",
    "        image, results = mediapipe_detection(imgScaled, holistic)\n",
    "        print(results)\n",
    "\n",
    "        # Disegna i landmark stilizzati sull'immagine\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Logica di predizione\n",
    "        keypoints = extract_keypoints(results)  # Estrai i landmark chiave\n",
    "\n",
    "        res = model.predict(np.expand_dims(keypoints, axis=0))[0]  # Effettua la predizione utilizzando il modello\n",
    "        print(actions[np.argmax(res)])  # Stampa l'azione corrispondente alla predizione\n",
    "        predictions.append(np.argmax(res))  # Aggiungi la predizione alla lista delle predizioni\n",
    "\n",
    "        if stateResult is False:\n",
    "            # Visualizza l'immagine del gesto corrente DA RICONOSCERE\n",
    "            if current_gesture < len(gesture_images):\n",
    "                gesture_image = gesture_images[current_gesture]\n",
    "                imgBG = cvzone.overlayPNG(imgBG, gesture_image, (149, 310))\n",
    "\n",
    "            # Avvia un timer di 3 secondi\n",
    "            timer = time.time() - initialTime\n",
    "            cv2.putText(imgBG, str(int(timer)), (605, 435), cv2.FONT_HERSHEY_PLAIN, 6, (255, 0, 255), 4)\n",
    "\n",
    "            if timer > 3:\n",
    "                stateResult = True\n",
    "                timer = 0\n",
    "\n",
    "                # Logica di visualizzazione\n",
    "                if np.unique(predictions[-10:])[0] == np.argmax(res) and res[np.argmax(res)] > threshold:\n",
    "                    if len(sentence) > 0:\n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "                if len(sentence) > 5:\n",
    "                    sentence = sentence[-5:]\n",
    "\n",
    "                # Verifica se il gesto corrente è stato indovinato\n",
    "                if len(sentence) > 0 and str(current_gesture) == sentence[-1]:\n",
    "                    prev_gesture = str(current_gesture)\n",
    "                    current_gesture += 1\n",
    "                    sentence = []\n",
    "                    indovinato = True\n",
    "\n",
    "                    # Mostra la scritta \"OTTIMO LAVORO\"\n",
    "                    show_message_on_image(imgBG, \"GOOD JOB\", (0, 255, 0))\n",
    "                    cv2.waitKey(500)\n",
    "\n",
    "                    # Controlla se è stato raggiunto l'ultimo gesto\n",
    "                    if current_gesture >= num_gestures:\n",
    "                        current_gesture = 0  # Ripristina l'indice del gesto\n",
    "                else:\n",
    "                    # Mostra la scritta \"RIPROVA\"\n",
    "                    show_message_on_image(imgBG, \"TRY AGAIN\", (0, 0, 255))\n",
    "                    cv2.waitKey(500)\n",
    "\n",
    "        imgBG[234:654, 795:1195] = image  # Sovrapponi l'immagine dei landmark allo sfondo\n",
    "\n",
    "        if stateResult:\n",
    "            imgBG = cvzone.overlayPNG(imgBG, gesture_image, (149, 310))  # Sovrapponi l'immagine del gesto corrente\n",
    "\n",
    "        cv2.imshow(\"BG\", imgBG)  # Mostra l'immagine con lo sfondo\n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('s') or indovinato == True:\n",
    "            game_started = True\n",
    "            initialTime = time.time()\n",
    "            stateResult = False\n",
    "            indovinato = False\n",
    "\n",
    "        # Interrompi in modo corretto\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            initialTime = time.time()\n",
    "            exit_flag = True  # Imposta il flag di uscita\n",
    "\n",
    "    cap.release()  # Rilascia la cattura video dalla webcam\n",
    "    cv2.destroyAllWindows()  # Chiudi tutte le finestre\n",
    "    cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('sol3D')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "487071e4b78db7dea0d63908693b0450144b2a88b91c498155ab01fac23904db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
