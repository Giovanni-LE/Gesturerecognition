# Gesturerecognition

# Landmark-based Hand Gesture Recognition using MediaPipe: Investigating the Accuracy of Depth Estimation

This repository contains the code and resources for a landmark-based hand gesture recognition project using MediaPipe. The main focus of this project was to investigate the accuracy of depth estimation in MediaPipe and utilize it to develop a game for teaching hand gestures to deaf children from 0 to 5.

## Gesture Recognition Model
To accomplish our goal, we leveraged the landmark data generated by the MediaPipe algorithm to train a feed-forward neural network for gesture classification. The model architecture we used is as follows:


```
model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(126,)))
model.add(Dense(32, activation='relu'))
model.add(Dense(actions.shape[0], activation='softmax'))
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
```

The model consists of three dense layers, with 64 and 32 units respectively, using the ReLU activation function. The input shape is set to (126,), and the output layer employs the softmax activation function to classify the gestures from 0 to 5, including an additional class for "other" gestures. We trained the model using a dataset we created, which consisted of 3,600 frames for each gesture, resulting in a total of 14,400 frames. We split the dataset into 5% for testing and the remaining for training. The trained model achieved an accuracy of 0.99.

### Loss and Accuracy Plots

## Real-time Gesture Recognition
After training and testing the model, we developed a game specifically designed for deaf children. The game's interface consists of two windows: "NUMBER" and "PLAYER." The "NUMBER" window displays images of numbers that the child needs to replicate in order to progress and view the next image. Adjacent to the "NUMBER" window is the "PLAYER" window, which displays the child's webcam feed. Between the two windows, there is a timer that counts from 0 to 3. Once the timer reaches 3, the program captures the child's gesture to determine if it matches the target gesture. If the gesture is correctly reproduced, the game proceeds to the next image. The child can choose to exit the game by pressing the "Q" key, which will close the program.

### Game Interface

## Depth Estimation Evaluation
In addition to the gesture recognition game, we also aimed to evaluate the accuracy of depth estimation in MediaPipe using an Intel RealSense RGB-D camera. For each gesture, we calculated the mean absolute errors (in meters) and the standard deviation. We created separate graphs for each gesture, illustrating the average errors and standard deviations.

### Error and Standard Deviation Graphs

To enhance visualization, we represented the errors on a skeletonized hand. The landmarks were visualized using circles with radii equivalent to the mean error for each landmark.

### Hand Images with Mean Errors

## Conclusion
This project successfully explored the accuracy of depth estimation in MediaPipe and utilized it to develop a game for teaching hand gestures to deaf children. The trained neural network achieved a high accuracy of 0.99 in recognizing gestures from 0 to 5, including an "other" class. The real-time game interface provides an interactive and engaging learning experience for children. Additionally, the evaluation of depth estimation through the Intel RealSense camera sheds light on the accuracy of MediaPipe's landmark depth estimation.
